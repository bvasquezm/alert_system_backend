"""
Orquestador de scrapers para ejecutar jobs en paralelo por cada pa√≠s
Utiliza ThreadPoolExecutor para ejecutar m√∫ltiples scrapers de forma concurrente
Guarda alertas en MongoDB
"""
from concurrent.futures import ThreadPoolExecutor, as_completed
from src.scraper import ComponentScraper
from src.storage import AlertStorage
import json
from datetime import datetime
from typing import Dict, List
import os
from dotenv import load_dotenv
import logging

load_dotenv()

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ScraperOrchestrator:
    """Orquestador de scrapers para ejecuci√≥n paralela por pa√≠s"""
    
    def __init__(self, config_path: str = "config_components.json", headless: bool = True, max_workers: int = None, mongo_uri: str = None):
        """
        Inicializa el orquestador

        """
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.config_path = config_path
        self.headless = headless
        self.max_workers = max_workers or len(self.config)
        self.results = []
        self.alerts = []
        
        # Inicializar storage (MongoDB)
        self.mongo_uri = mongo_uri or os.getenv('MONGO_URI', 'mongodb://admin:password@mongodb:27017/scraper_alerts')
        try:
            print(f"  üîó Conectando a MongoDB: {self.mongo_uri}")
            self.storage = AlertStorage(mongo_uri=self.mongo_uri)
            self.storage_available = True
            print(f"  ‚úÖ Storage MongoDB listo")
        except Exception as e:
            logger.warning(f"No se pudo conectar a MongoDB: {e}. Las alertas se guardar√°n solo en memoria.")
            print(f"  ‚ùå MongoDB NO disponible: {e}")
            self.storage = None
            self.storage_available = False
        
        print(f"\n{'='*70}")
        print("INICIALIZANDO ORQUESTADOR DE SCRAPERS")
        print(f"{'='*70}")
        print(f"  üìç Pa√≠ses configurados: {', '.join(self.config.keys())}")
        print(f"  üîÑ Workers paralelos: {self.max_workers}")
        print(f"  üé¨ Modo headless: {'‚úì S√≠' if self.headless else '‚úó No'}")
        print(f"  üíæ Storage: {'MongoDB ‚úÖ' if self.storage_available else 'En memoria ‚ö†Ô∏è'}")
        print(f"{'='*70}\n")
    
    def _print_summary(self, report: Dict) -> None:
        print(f"\n{'='*70}")
        print("RESUMEN DE EJECUCI√ìN")
        print(f"{'='*70}")
        print(f"  ‚è±Ô∏è  Tiempo total: {report['execution_time']}")
        print(f"  ‚úÖ Pa√≠ses exitosos: {report['successful']}/{report['total_countries']}")
        print(f"  ‚ùå Pa√≠ses fallidos: {report['failed']}/{report['total_countries']}")
        print(f"  ‚ö†Ô∏è  Alertas generadas: {report['total_alerts']}")
        if self.storage_available:
            print(f"  üíæ Alertas guardadas en MongoDB: {report['total_alerts']}")
            try:
                alerts_in_db = self.storage.load_alerts()
                print(f"  üîç Alertas en BD: {len(alerts_in_db)}")
            except Exception:
                pass
        print(f"{'='*70}\n")

    def _scrape_country(self, country: str) -> Dict:
        """
        Ejecuta el scraping para un pa√≠s espec√≠fico
        
        Args:
            country: C√≥digo de pa√≠s a scrapear
            
        Returns:
            Diccionario con resultados del scraping
        """
        print(f"\n‚ñ∂Ô∏è  Iniciando job para {country.upper()}")
        
        try:
            scraper = ComponentScraper(self.config_path)
            scraper.headless = self.headless
            
            country_config = self.config.get(country, {})
            results_by_page = []
            country_alerts = []
            
            for page_type, page_config in country_config.items():
                if page_type == 'setup_product_url':
                    continue  # Saltar configuraci√≥n especial
                
                url = page_config.get('url_example')
                if not url:
                    logger.warning(f"No se encontr√≥ URL para {country}/{page_type}")
                    continue
                
                print(f"  üìÑ Scrapeando {page_type}...")
                
                try:
                    scraper.fetch_page(url)
                    result = scraper.scrape_page(
                        country,
                        page_type,
                        url
                    )
                    results_by_page.append(result)
                    
                    if result and 'error' not in result:
                        print(f"    ‚ÑπÔ∏è  Procesando resultado del scraping...")
                        print(f"    ‚ÑπÔ∏è  Keys en resultado: {result.keys()}")
                        
                        try:
                            page_alerts = scraper.generate_alerts(result)
                            print(f"    ‚ÑπÔ∏è  generate_alerts() retorn√≥: {type(page_alerts)}, longitud: {len(page_alerts) if page_alerts else 0}")
                            
                            if page_alerts and len(page_alerts) > 0:
                                print(f"    üìç Alertas generadas: {len(page_alerts)}")
                                for alert in page_alerts:
                                    if 'status' not in alert:
                                        alert['status'] = 'MISSING_COMPONENT'
                                    if 'timestamp' not in alert:
                                        alert['timestamp'] = datetime.now().isoformat()
                                    country_alerts.append(alert)
                                    print(f"      ‚ûï Alerta: {alert.get('component')} - {alert.get('message')}")
                            else:
                                print(f"    ‚úÖ No hay alertas para esta p√°gina (todo OK)")
                        except Exception as e:
                            print(f"    ‚ùå Error llamando generate_alerts(): {e}")
                            import traceback
                            traceback.print_exc()
                    else:
                        if 'error' in result:
                            print(f"    ‚ùå Error en scraping: {result['error']}")
                        else:
                            print(f"    ‚ö†Ô∏è  No se pudo procesar el resultado")
                    
                except Exception as e:
                    logger.error(f"Error scrapeando {page_type}: {e}")
                    print(f"    ‚ùå Error: {e}")
            
            print(f"\n  üìä RESUMEN {country.upper()}:")
            print(f"     ‚Ä¢ Total alertas generadas: {len(country_alerts)}")
            print(f"     ‚Ä¢ Storage disponible: {'S√ç ‚úÖ' if self.storage_available else 'NO ‚ùå'}")
            
            # GUARDADO DE ALERTAS EN MONGODB SI EST√Å DISPONIBLE
            if len(country_alerts) > 0:
                if self.storage_available:
                    try:
                        print(f"     üì§ Guardando {len(country_alerts)} alertas...")
                        self.storage.add_alerts(country_alerts)
                        print(f"     ‚úÖ Guardadas exitosamente en MongoDB")
                    except Exception as e:
                        logger.error(f"Error guardando alertas en MongoDB: {e}")
                        print(f"     ‚ùå ERROR al guardar: {e}")
                else:
                    print(f"     ‚ö†Ô∏è  MongoDB no disponible - alertas NO se guardar√°n")
            else:
                print(f"     ‚ÑπÔ∏è  Sin alertas para guardar")
            
            self.alerts.extend(country_alerts)

            scraper._close_browser()
            
            print(f"‚úÖ Job completado para {country.upper()}")
            
            return {
                'country': country,
                'status': 'success',
                'alerts_count': len(country_alerts),
                'pages': results_by_page,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error en job para {country.upper()}: {e}")
            print(f"‚ùå Error en job para {country.upper()}: {e}")
            return {
                'country': country,
                'status': 'failed',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def run(self) -> Dict:
        """
        Ejecuta todos los jobs de scraping en paralelo
        
        Returns:
            Diccionario con resultados agregados de todos los pa√≠ses
        """
        print(f"\n{'='*70}")
        print("EJECUTANDO JOBS EN PARALELO")
        print(f"{'='*70}\n")
        
        start_time = datetime.now()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            promise_results = {
                executor.submit(self._scrape_country, country): country 
                for country in self.config.keys()
            }
            
            for p_result in as_completed(promise_results):
                country = promise_results[p_result]
                try:
                    result = p_result.result()
                    self.results.append(result)
                except Exception as e:
                    logger.error(f"Excepci√≥n en job para {country}: {e}")
                    self.results.append({
                        'country': country,
                        'status': 'failed',
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    })
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        minutes, seconds = divmod(duration, 60)
        
        report = {
            'execution_time': f"{int(minutes)}m {int(seconds)}s",
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'total_countries': len(self.config),
            'successful': sum(1 for r in self.results if r['status'] == 'success'),
            'failed': sum(1 for r in self.results if r['status'] == 'failed'),
            'total_alerts': len(self.alerts),
            'results': self.results
        }
        
        self._print_summary(report)
        
        return report


if __name__ == "__main__":
    orchestrator = ScraperOrchestrator(
        config_path="config_components.json",
        headless=True,  # Ejecutar en modo headless
        max_workers=None  # Autom√°tico (uno por pa√≠s)
    )
    
    # Ejecutar todos los jobs en paralelo
    report = orchestrator.run()
